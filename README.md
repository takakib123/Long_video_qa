# Long_video_qa


Long Video question answering(LVQA) is a challenging task for vision language models(VLMs). Long Videos contain thousand of frames. LVQA generally requires a frame selection or token reduction strategy to reduce the number of input frames to VLMs. In this project, we take a state of the art frame selection method [Videotree](https://arxiv.org/abs/2405.19209) and improve it in a training free manner for the [egoschema](https://arxiv.org/abs/2308.09126) test set.

## Method

